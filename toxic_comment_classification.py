# -*- coding: utf-8 -*-
"""toxic-comment-classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OsiEvLdn-MwJru11xTYqlTdRNhi8u_c8
"""

!wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip

!unzip wiki-news-300d-1M.vec.zip

import numpy as np
import pandas as pd 
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud, STOPWORDS
import re
import tqdm

# plt.style.use('seaborn')

data = pd.read_csv("/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv.zip")
data.head()

data.info()

categories = list(data.columns.values)
categories = categories[2:]
print(categories)

"""## **EDA**"""

counts = []
for category in categories:
    counts.append((category, data[category].sum()))
df_stats = pd.DataFrame(counts, columns=['category', 'number of comments'])
df_stats

plt.figure(figsize=(10,6))
sns.barplot(x = list(data.columns[2:]), y= list(data.iloc[:,2:].sum()))
plt.title('Total comments in each category')
plt.xlabel('Number of comments')
plt.ylabel('Types of comments')

count = 0
for i in range(len(data)):
    n = data.iloc[i,2:].sum()
    if n > 1:
        count = count + 1
    
print('Total multi-labeled comments:',count)

fig = plt.figure(figsize = (10,8))
sns.heatmap(data.corr(), annot=True,cmap="Blues")
plt.suptitle('Heatmap of Training label Class Correlation',size = 14)
plt.xlabel("Classes")
plt.ylabel("Classes")
plt.show()

def word_plot(column,text):
    
    comments = data['comment_text'].loc[column == 1].values     # sort by toxicity
    
    word_cloud = WordCloud( width = 640, height = 640, background_color = 'black',
                stopwords = STOPWORDS).generate(str(comments))     # stopwords are a,an,the
    
    fig = plt.figure( figsize = (15, 7), facecolor = 'k', edgecolor = 'k')
    plt.subplot()
    plt.imshow(word_cloud.recolor(colormap="Blues"), interpolation = 'bilinear')
    plt.suptitle("Most frequent words in " +  text , y = 1.06,color = "white")
    plt.tight_layout(pad = 0)
    plt.axis('off')
    plt.show()

word_plot(data['toxic'], "Toxic comments")
word_plot(data['severe_toxic'],"severe Toxic comments")
word_plot(data['obscene'],"Obscene comments")
word_plot(data['insult'],"Insult comments")
word_plot(data['threat'],"Threat comments")
word_plot(data['identity_hate'],"Identity Hate comments")

"""## **Data Cleaning**"""

# creating new column named "total_classes" which will show total no. of classes comment belongs to

data["total_classes"] = data.iloc[:,2:].apply(lambda x: sum(x), axis=1)   
data

# Conerting all strings into lower case
data["comment_text"] = data["comment_text"].str.lower()

# Replacing non-breaking space with a regular space 
data["comment_text"] = data["comment_text"].str.replace("\xa0", " ", regex=False).str.split().str.join(" ")

# Removing extra spaces in text
data["comment_text"] = data["comment_text"].map(lambda x: re.sub(r"\s\s+", " ",x))

# Removing Hyperlinks from text
data["comment_text"] = data["comment_text"].map(lambda x: re.sub(r"https?://\S+|www\.\S+","",x))

# Removing Special characters 
data["comment_text"] = data["comment_text"].map(lambda x: re.sub(r"[^a-zA-Z0-9\s\"\',:;?!.()]", " ",x))

counts = []
for category in categories:
    counts.append((category, data[category].sum()))
df_stats = pd.DataFrame(counts, columns=['category', 'number of comments'])
df_stats

"""## **Data Training**"""

import gensim.models.keyedvectors as word2vec
import pandas as pd
import gc
from keras_preprocessing.text import Tokenizer
from keras_preprocessing.sequence import pad_sequences
from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation
from keras.layers import CuDNNLSTM
from keras.layers import Bidirectional, GlobalMaxPool1D
from keras.models import Model
from keras.models import Sequential
from tensorflow.keras.optimizers import Adam
from tqdm import tqdm

train = data
test = pd.read_csv("/kaggle/input/jigsaw-toxic-comment-classification-challenge/test.csv.zip")
test_labels = pd.read_csv("../input/jigsaw-toxic-comment-classification-challenge/test_labels.csv.zip")
test = test.merge(test_labels, on="id")

# Drop all rows with -1 in label values
drop_idxs = test[
    (test.toxic == -1) | (test.severe_toxic == -1) | (test.obscene == -1) | 
    (test.threat == -1) | (test.insult == -1) | (test.identity_hate == -1)
].index
test = test.drop(drop_idxs, axis="rows")

list_sentences_train = train["comment_text"]
list_sentences_test = test["comment_text"]
y_train = train[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].values
y_test = test[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].values

embed_size = 100
max_features = 20000
max_len = 200

tokenizer = Tokenizer(num_words=max_features)
tokenizer.fit_on_texts(list(list_sentences_train))
list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)
list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)
X_train = pad_sequences(list_tokenized_train, maxlen=max_len)
X_test = pad_sequences(list_tokenized_test, maxlen=max_len)

def loadEmbeddingMatrix(typeToLoad):
        # load different embedding file from Kaggle depending on which embedding 
        # matrix we are going to experiment with

        EMBEDDING_FILE='/kaggle/working/wiki-news-300d-1M.vec'
        embed_size = 300
        embeddings_index = dict()
        # Transfer the embedding weights into a dictionary by iterating through every line of the file.
        f = open(EMBEDDING_FILE)
        for line in tqdm(f):
            try:
                # split up line into an indexed array
                values = line.split()
                # first index is word
                word = values[0]
                # store the rest of the values in the array as a new array
                coefs = np.asarray(values[1:], dtype='float32')
                # ignore values without headers as word
                if len(coefs)==300:
                    embeddings_index[word] = coefs #300 dimension
                else:
                    continue
            except:
                # split up line into an indexed array
                values = line.split()
                # first index is word or sometimes the second as well
                word = values[1]
                # store the rest of the values in the array as a new array
                coefs = np.asarray(values[2:], dtype='float32')
                print(values[:2])
                print(len(coefs))
                embeddings_index[word] = coefs #300 dimension
        f.close()
        print('Loaded %s word vectors.' % len(embeddings_index))
        #return embeddings_index
            
        gc.collect()
        # We get the mean and standard deviation of the embedding weights so that we could maintain the 
        # same statistics for the rest of our own random generated weights. 
        all_embs = np.stack(embeddings_index.values())
        #np.stack(list(embeddings_index.values()))
        emb_mean, emb_std = all_embs.mean(), all_embs.std()
        
        nb_words = len(tokenizer.word_index)
        # We are going to set the embedding size to the pretrained dimension as we are replicating it.
        # the size will be Number of Words in Vocab X Embedding Size
        embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))
        gc.collect()

        # With the newly created embedding matrix, we'll fill it up with the words that we have in both 
        # our own dictionary and loaded pretrained embedding. 
        embeddedCount = 0
        for word, i in tokenizer.word_index.items():
            i-=1
            # then we see if this word is in glove's dictionary, if yes, get the corresponding weights
            embedding_vector = embeddings_index.get(word)
            # and store inside the embedding matrix that we will train later on.
            if embedding_vector is not None: 
                embedding_matrix[i] = embedding_vector
                embeddedCount+=1
        print('total embedded:',embeddedCount,'common words')
        
        del(embeddings_index)
        gc.collect()
        
        # finally, return the embedding matrix
        return embedding_matrix

embedding_matrix = loadEmbeddingMatrix('fasttext')

embedding_matrix.shape

model = Sequential()
model.add(Embedding(len(tokenizer.word_index), embedding_matrix.shape[1], 
                  weights=[embedding_matrix], trainable=True, input_shape=(max_len,)))
model.add(Bidirectional(CuDNNLSTM(64, return_sequences=True, name='lstm_layer')))
model.add(Dropout(0.1))
model.add(GlobalMaxPool1D())
model.add(Dropout(0.1))
model.add(Dense(50, activation="relu"))
model.add(Dropout(0.1))
model.add(Dense(6, activation="sigmoid"))
model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=1e-3), metrics=['accuracy'])

model.summary()

batch_size = 32
epochs = 5
trace = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)

# extract the accuracy and loss from the history object
acc = trace.history['accuracy']
val_acc = trace.history['val_accuracy']
loss = trace.history['loss']
val_loss = trace.history['val_loss']

# create a figure and axes
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))

# plot the accuracy on the first axes
ax1.plot(acc, label='Training Accuracy')
ax1.plot(val_acc, label='Validation Accuracy')
ax1.set_title('Accuracy')
ax1.legend()

# plot the loss on the second axes
ax2.plot(loss, label='Training Loss')
ax2.plot(val_loss, label='Validation Loss')
ax2.set_title('Loss')
ax2.legend()

# show the plot
plt.show()

test_loss, test_acc = model.evaluate(X_test, y_test)
print("Test Loss:", test_loss)
print("Test Accuracy:", test_acc)